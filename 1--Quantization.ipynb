{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *   #adds ~107MB\n",
    "from fastai.text import *    #adds ~6MB\n",
    "from fastai.callbacks.tracker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "PATH = Path('data/IAM_handwriting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    sz = os.path.getsize(\"temp.p\")/1e6\n",
    "    os.remove('temp.p')\n",
    "    return sz\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    print('%.2f MB' % size_of_model(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## word combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fname = 'edited_sm_synth.csv'\n",
    "CSV = PATH/fname\n",
    "FOLDER = 'edited_sm_synth'\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sz,bs = 256,50\n",
    "seq_len,word_len = 100,50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## font generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#font generated\n",
    "fname = 'font_mix_129k.csv'\n",
    "FOLDER = 'combo_cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CSV = PATH/fname\n",
    "df = pd.read_csv(CSV)\n",
    "\n",
    "sz,bs = 512,15\n",
    "seq_len,word_len = 750,300\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## handwriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#handwriting only\n",
    "fname = 'hand_mix_25k.csv'\n",
    "FOLDER = 'combo_cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CSV = PATH/fname\n",
    "df = pd.read_csv(CSV)\n",
    "\n",
    "sz,bs = 512,15\n",
    "seq_len,word_len = 750,300\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fname = 'edited_pg.csv'\n",
    "FOLDER = 'paragraphs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CSV = PATH/fname\n",
    "df = pd.read_csv(CSV)\n",
    "\n",
    "sz,bs = 512,15\n",
    "seq_len,word_len = 750,300\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=False, max_zoom=1, max_rotate=2, max_warp=0.1, max_lighting=0.5)\n",
    "\n",
    "def force_gray(image): return image.convert('L').convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# sp = spm.SentencePieceProcessor()\n",
    "# sp.Load(str(PATH/'spm_full_10k.model'))\n",
    "# sp.SetEncodeExtraOptions(\"eos\")\n",
    "# sp.SetDecodeExtraOptions(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def add_cap_tokens(text):  # before encode\n",
    "    re_caps = re.compile(r'[A-Z]+')\n",
    "    return re_caps.sub(_replace_caps, text)\n",
    "    \n",
    "def _replace_caps(m):\n",
    "    tok = '[UP]' if m.end()-m.start() > 1 else '[MAJ]'\n",
    "    return tok + m.group().lower()\n",
    "\n",
    "def remove_cap_tokens(text):  # after decode\n",
    "    text = re.sub(r'\\[UP\\]\\w+', lambda m: m.group()[4:].upper(), text)  #cap entire word\n",
    "    text = re.sub(r'\\[MAJ\\]\\w?', lambda m: m.group()[5:].upper(), text) #cap first letter\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word only (export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def label_collater(samples:BatchSamples, pad_idx:int=0):\n",
    "    \"Function that collect samples and pads ends of labels.\"\n",
    "    data = to_data(samples)\n",
    "    ims, lbls = zip(*data)\n",
    "    imgs = torch.stack(list(ims))\n",
    "    if len(data) is 1 and lbls[0] is 0:   #predict\n",
    "        labels = torch.zeros(1,1).long()\n",
    "        return imgs, labels    \n",
    "    max_len = max([len(s) for s in lbls])\n",
    "    labels = torch.zeros(len(data), max_len+1).long() + pad_idx  # add 1 to max_len to account for bos token\n",
    "    for i,lbl in enumerate(lbls):\n",
    "        labels[i,:len(lbl)] = torch.from_numpy(lbl)  #padding end    \n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class SPTokenizer(BaseTokenizer):\n",
    "    \"Wrapper around a SentncePiece tokenizer to make it a `BaseTokenizer`.\"\n",
    "    def __init__(self, model_prefix:str):\n",
    "        self.tok = spm.SentencePieceProcessor()\n",
    "        self.tok.Load(f'{model_prefix}.model')\n",
    "        self.tok.SetEncodeExtraOptions(\"eos\")\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        return self.tok.EncodeAsIds(t)[1:]\n",
    "      \n",
    "class CustomTokenizer():\n",
    "    def __init__(self, tok_func:Callable, model_prefix:str):\n",
    "        self.tok_func, self.model_prefix = tok_func,model_prefix\n",
    "        self.pre_rules = [rm_useless_spaces, add_cap_tokens]\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        res = f'Tokenizer {self.tok_func.__name__} using `{self.model_prefix}` model with the following rules:\\n'\n",
    "        for rule in self.pre_rules: res += f' - {rule.__name__}\\n'\n",
    "        return res        \n",
    "\n",
    "    def process_one(self, t:str, tok:BaseTokenizer) -> List[str]:\n",
    "        \"Processe one text `t` with tokenizer `tok`.\"\n",
    "        for rule in self.pre_rules: t = rule(t)  \n",
    "        toks = tok.tokenizer(t) \n",
    "        return toks \n",
    "                                                                         \n",
    "    def process_all(self, texts:Collection[str]) -> List[List[str]]: \n",
    "        \"Process a list of `texts`.\" \n",
    "        tok = self.tok_func(self.model_prefix)\n",
    "        return [self.process_one(t, tok) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class SPList(ItemList):\n",
    "    def __init__(self, items:Iterator, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        model_prefix = self.path/'spm_full_10k'\n",
    "        cust_tok = CustomTokenizer(SPTokenizer, model_prefix)\n",
    "        self.processor = TokenizeProcessor(tokenizer=cust_tok, include_bos=False)\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(str(model_prefix)+'.model')\n",
    "        self.sp.SetDecodeExtraOptions(\"bos:eos\")\n",
    "        \n",
    "        self.pad_idx = 0\n",
    "        self.copy_new += ['sp']\n",
    "    \n",
    "    def get(self, i):\n",
    "        o = self.items[i]\n",
    "        return Text(o, self.textify(o))\n",
    "    \n",
    "    def reconstruct(self, t:Tensor):\n",
    "        nonzero_idxs = (t != self.pad_idx).nonzero()\n",
    "        idx_min = 0\n",
    "        idx_max = nonzero_idxs.max() if len(nonzero_idxs) > 0 else 0\n",
    "        return Text(t[idx_min:idx_max+1], self.textify(t[idx_min:idx_max+1]))\n",
    "\n",
    "    def analyze_pred(self, pred:Tensor):\n",
    "        return torch.argmax(pred, dim=-1)        \n",
    "    \n",
    "    def textify(self, ids):\n",
    "        if isinstance(ids, torch.Tensor): ids = ids.tolist()\n",
    "        st = self.sp.DecodeIds(ids)\n",
    "        st = remove_cap_tokens(st)\n",
    "        return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "data = (ImageList.from_df(df, path=PATH, folder=FOLDER, after_open=force_gray)    # ~245MB\n",
    "        .split_by_rand_pct(valid_pct=0.15, seed=42)\n",
    "        .label_from_df(label_cls=SPList)\n",
    "        .transform(tfms, size=sz, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=bs, device=device, collate_fn=label_collater)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export only the data for use with the final jit graphed model\n",
    "# can't export them together because TorchScript model doesn't allow pickling\n",
    "\n",
    "# data.valid_ds.export('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "# data.show_batch(rows=2, ds_type=DatasetType.Train, figsize=(18,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = 'uploads'\n",
    "df = pd.read_csv(PATH/'uploads.csv')\n",
    "len(df)\n",
    "\n",
    "sz,bs = 512,14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = 'paragraphs'\n",
    "df = pd.read_csv(PATH/'test_pg.csv')\n",
    "len(df)\n",
    "\n",
    "sz,bs = 512,15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = (ImageList.from_df(df, path=PATH, folder=FOLDER, after_open=force_gray)    # ~245MB\n",
    "        .split_none()\n",
    "        .label_from_df(label_cls=SPList)\n",
    "        .transform([], size=sz, resize_method=ResizeMethod.SQUISH)\n",
    "        .databunch(bs=bs, device=device, collate_fn=label_collater)\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "# LayerNorm = nn.LayerNorm\n",
    "LayerNorm = partial(nn.LayerNorm, eps=1e-4)  # eps: 1e-4 accomodates mixed precision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "# class SublayerConnection(nn.Module):\n",
    "#     \"A residual connection followed by a layer norm.  Note: (for code simplicity) norm is first.\"\n",
    "#     def __init__(self, size, dropout):\n",
    "#         super(SublayerConnection, self).__init__()\n",
    "#         self.norm = LayerNorm(size)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x, sublayer):\n",
    "#         return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    __constants__ = ['layers']\n",
    "    \n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "# class EncoderLayer(nn.Module):\n",
    "#     \"Encoder: self-attn and feed forward\"\n",
    "#     def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "#         super(EncoderLayer, self).__init__()\n",
    "#         self.size = size\n",
    "#         self.self_attn = self_attn\n",
    "#         self.feed_forward = feed_forward\n",
    "#         self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "#         x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "#         return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    __constants__ = ['layers']\n",
    "    \n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, src, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "# class DecoderLayer(nn.Module):\n",
    "#     \"Decoder: self-attn, src-attn, and feed forward\"\n",
    "#     def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "#         super(DecoderLayer, self).__init__()\n",
    "#         self.size = size\n",
    "#         self.self_attn = self_attn\n",
    "#         self.src_attn = src_attn\n",
    "#         self.feed_forward = feed_forward\n",
    "#         self.sublayer = clones(SublayerConnection(size, dropout), 3)  # wraps layer in residual,dropout,norm\n",
    " \n",
    "#     def forward(self, x, src, tgt_mask=None):\n",
    "#         x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))  # acts as a weak LM\n",
    "#         x = self.sublayer[1](x, lambda x: self.src_attn(x, src, src))\n",
    "#         return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "# def attention(query, key, value, mask=None, dropout=None):\n",
    "#     \"Compute 'Scaled Dot Product Attention'\"\n",
    "#     depth = query.size(-1)\n",
    "#     scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(depth)\n",
    "#     if mask is not None:\n",
    "#         scores = scores.masked_fill(mask == 0, -1e4)  #changed from: -1e9 to accomodate mixed precision  \n",
    "#     p_attn = F.softmax(scores, dim=-1)\n",
    "#     if dropout is not None:\n",
    "#         p_attn = dropout(p_attn)\n",
    "#     return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "# class SingleHeadedAttention(nn.Module):\n",
    "#     def __init__(self, d_model, dropout=0.2):\n",
    "#         super(SingleHeadedAttention, self).__init__()\n",
    "#         self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "#         self.attn = None\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, query, key, value, mask=None):        \n",
    "#         query, key, value = [l(x) for l, x in zip(self.linears, (query, key, value))]\n",
    "#         x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "#         return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "# class MultiHeadedAttention(nn.Module):\n",
    "#     def __init__(self, d_model, h=8, dropout=0.2):\n",
    "#         super(MultiHeadedAttention, self).__init__()\n",
    "#         assert d_model % h == 0\n",
    "#         self.d_k = d_model // h        # assume d_v always equals d_k\n",
    "#         self.h = h\n",
    "#         self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "#         self.attn = None\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, q, k, v, mask=None):\n",
    "#         if mask is not None: mask = mask.unsqueeze(1)\n",
    "#         bs = q.size(0)\n",
    "        \n",
    "#         # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "#         q, k, v = [l(x).view(bs, -1, self.h, self.d_k).transpose(1,2) for l, x in zip(self.linears, (q, k, v))]\n",
    "        \n",
    "#         # 2) Apply attention on all the projected vectors in batch. \n",
    "#         x, self.attn = attention(q, k, v, mask=mask, dropout=self.dropout)\n",
    "        \n",
    "#         # 3) \"Concat\" using a view and apply a final linear. \n",
    "#         x = x.transpose(1, 2).contiguous().view(bs, -1, self.h * self.d_k)\n",
    "#         return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.2):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_model*4)\n",
    "        self.w_2 = nn.Linear(d_model*4, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.gelu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, dropout=0.1, max_len=2000):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0.0, max_len).unsqueeze(1)\n",
    "#         log_increment = math.log(1e4) / d_model\n",
    "#         div_term = torch.exp(torch.arange(0.0, d_model, 2) * -log_increment)  \n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe.unsqueeze_(0)\n",
    "\n",
    "#         self.register_buffer('pe', pe)    #(1,max_len,d_model)\n",
    "#         # registered buffers are Tensors (not Variables)\n",
    "#         # not a parameter but still want in the state_dict\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.pe[:, :x.size(1)]\n",
    "#         return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "# class Embeddings(nn.Module):\n",
    "#     def __init__(self, d_model, vocab):\n",
    "#         super(Embeddings, self).__init__()\n",
    "#         self.lut = nn.Embedding(vocab, d_model)\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Graph (via JIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## No Static Quantization (only dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ResnetBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        net = models.resnet18(True)\n",
    "        modules = list(net.children())[:-2]\n",
    "        self.base = nn.Sequential(*modules)                  #32x32 : 256\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.base(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust modules for Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.quantization import resnet18 as qresnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class ResnetBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        net = qresnet18(True)\n",
    "        modules = list(net.children())[:-4]\n",
    "        self.base = nn.Sequential(*modules)                  #32x32 : 256\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.base(x)\n",
    "        return self.dequant(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust modules for TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbeddings(nn.Module):\n",
    "    __constants__ = ['nl_tok', 'd_model']\n",
    "    \n",
    "    def __init__(self, d_model, vocab, dropout=0.0):\n",
    "        super(LearnedPositionalEmbeddings, self).__init__()\n",
    "        self.nl_tok  = 4\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embed = nn.Embedding(vocab, d_model, 0)\n",
    "        self.rows = nn.Embedding(15, d_model//2, 0)\n",
    "        self.w_cols = nn.Embedding(60, d_model//2, 0)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):  # x: [bs, sl]\n",
    "        rows,cols = self.encode_spatial_positions(x)\n",
    "        \n",
    "        row_t = self.rows(rows)            \n",
    "        col_t = self.w_cols(torch.clamp(cols, max=self.w_cols.num_embeddings-1))  # clamp to max column value\n",
    "        pos_enc = torch.cat((row_t, col_t), dim=-1)\n",
    "                \n",
    "        x = self.embed(x)\n",
    "        x = (x + pos_enc) * math.sqrt(self.d_model)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    def encode_spatial_positions(self, x):\n",
    "        rows,cols = torch.zeros_like(x),torch.zeros_like(x)\n",
    "        ii = 0\n",
    "        for batch in x.unbind():\n",
    "            nls = torch.nonzero(batch==self.nl_tok).flatten()\n",
    "            last = torch.nonzero(batch).flatten()[-1][None]\n",
    "            splits = torch.cat([nls,last])\n",
    "\n",
    "            p=0\n",
    "            for i in range(len(splits)):\n",
    "                n = int(splits[i].item())\n",
    "                rows[ii,p:n+1] = i+1\n",
    "                cols[ii,p:n+1] = torch.arange(1,n-p+2)\n",
    "                p = n+1\n",
    "\n",
    "            ii+=1\n",
    "        return rows,cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lpe = LearnedPositionalEmbeddings(512, 10000)\n",
    "# lpe.load_state_dict(lpe_sd)\n",
    "# lpe.eval()\n",
    "# lpe_trace = torch.jit.script(lpe)\n",
    "\n",
    "# torch.testing.assert_allclose(lpe(y), lpe_trace(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class Adaptor(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2,3).permute(0,2,1)\n",
    "        return x.mul(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    __constants__ = ['h', 'd_k']\n",
    "    \n",
    "    def __init__(self, d_model, h=8, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h        # assume d_v always equals d_k\n",
    "        self.h = h\n",
    "\n",
    "        self.linear0 = nn.Linear(d_model, d_model)\n",
    "        self.linear1 = nn.Linear(d_model, d_model)\n",
    "        self.linear2 = nn.Linear(d_model, d_model)\n",
    "        self.linear3 = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask:Optional[torch.Tensor]=None):\n",
    "        if mask is not None: mask = mask.unsqueeze(1)\n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        q = self.linear0(q).view(bs, -1, self.h, self.d_k).transpose(1,2)\n",
    "        k = self.linear1(k).view(bs, -1, self.h, self.d_k).transpose(1,2)\n",
    "        v = self.linear2(v).view(bs, -1, self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x = self.attention(q, k, v, mask=mask)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous().view(bs, -1, self.h * self.d_k)\n",
    "        return self.linear3(x)\n",
    "    \n",
    "    def attention(self, query, key, value, mask:Optional[torch.Tensor]=None):\n",
    "        depth = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(depth)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e4)  #changed from: -1e9 to accomodate mixed precision  \n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        return torch.matmul(self.dropout(p_attn), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex = torch.rand(2,3,512)\n",
    "\n",
    "# mha = MultiHeadedAttention(512).eval()   #.eval() here is critical otherwise dropout still adds randomness\n",
    "# mha_trace = torch.jit.script(mha)\n",
    "\n",
    "# torch.testing.assert_allclose(mha(ex,ex,ex), mha_trace(ex,ex,ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        \n",
    "        self.norm0 = LayerNorm(size)\n",
    "        self.dropout0 = nn.Dropout(dropout)\n",
    "        self.norm1 = LayerNorm(size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.norm0(x)\n",
    "        x = x + self.dropout0(self.self_attn(y,y,y))\n",
    "        \n",
    "        y = self.norm1(x)\n",
    "        x = x + self.dropout1(self.feed_forward(y))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff = PositionwiseFeedForward(512)\n",
    "# enc = EncoderLayer(512, mha, ff, 0.1).eval()\n",
    "# enc_trace = torch.jit.script(enc)\n",
    "\n",
    "# torch.testing.assert_allclose(enc(ex), enc_trace(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        \n",
    "        self.norm0 = LayerNorm(size)\n",
    "        self.dropout0 = nn.Dropout(dropout)\n",
    "        self.norm1 = LayerNorm(size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm2 = LayerNorm(size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "         \n",
    "    def forward(self, x, src, mask):\n",
    "        y = self.norm0(x)\n",
    "        x = x + self.dropout0(self.self_attn(y,y,y,mask))\n",
    "        \n",
    "        y = self.norm1(x)\n",
    "        x = x + self.dropout1(self.src_attn(y,src,src))\n",
    "        \n",
    "        y = self.norm2(x)\n",
    "        x = x + self.dropout2(self.feed_forward(y))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "# remove explicit calls to device for scripting (inference on CPU only)\n",
    "\n",
    "def rshift(tgt, bos_token:int=1):\n",
    "    \"Shift y to the right by prepending token\"\n",
    "    bos = torch.zeros((tgt.size(0),1)).type_as(tgt) + bos_token\n",
    "    return torch.cat((bos, tgt[:,:-1]), dim=-1)\n",
    "\n",
    "def subsequent_mask(size:int):\n",
    "    return torch.tril(torch.ones((1,size,size)).byte())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class WordTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embeddings, generator):\n",
    "        super(WordTransformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.w_decoder = decoder\n",
    "        self.embed = embeddings\n",
    "        self.generator = generator\n",
    "            \n",
    "    def forward(self, src, tgt):\n",
    "        tgt = rshift(tgt, 1).long()\n",
    "        mask = subsequent_mask(tgt.size(-1))\n",
    "        return self.w_decoder(self.embed(tgt), self.encoder(src), mask)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def generate(self, outs):\n",
    "        return self.generator(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def make_full_model(vocab, d_model, N=4, drops=0, heads=8):\n",
    "    c = deepcopy\n",
    "    attn = MultiHeadedAttention(d_model, heads)\n",
    "    ff = PositionwiseFeedForward(d_model, drops)\n",
    "    \n",
    "    model = WordTransformer(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), drops), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), drops), N),\n",
    "        LearnedPositionalEmbeddings(d_model, vocab, drops),  #word\n",
    "        nn.Linear(d_model, vocab),\n",
    "    )\n",
    "        \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class Img2Seq(nn.Module):\n",
    "    def __init__(self, img_encoder, adaptor, transformer):\n",
    "        super(Img2Seq, self).__init__()\n",
    "        self.img_enc = img_encoder\n",
    "        self.adaptor = adaptor\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def forward(self, src, seq_len:int=300):\n",
    "        feats = self.transformer.encoder(self.adaptor(self.img_enc(src)))\n",
    "        tgt = torch.ones((src.size(0),1), dtype=torch.long)\n",
    "\n",
    "        res = []\n",
    "        for i in range(seq_len):\n",
    "            emb = self.transformer.embed(tgt)\n",
    "            mask = subsequent_mask(tgt.size(-1))\n",
    "            dec_outs = self.transformer.w_decoder(emb, feats, mask)\n",
    "            prob = self.transformer.generate(dec_outs[:,-1])\n",
    "            res.append(prob)\n",
    "            pred = torch.argmax(prob, dim=-1, keepdim=True)\n",
    "            if (pred==0).all(): break\n",
    "            tgt = torch.cat([tgt,pred], dim=-1)\n",
    "        return torch.stack(res).transpose(1,0).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def make_model(vocab_len, d_model, em_sz, N=4, drops=0.1, heads=8):\n",
    "    img_encoder = ResnetBase()\n",
    "    adaptor = Adaptor()\n",
    "    transformer = make_full_model(vocab_len, d_model, N, drops, heads)\n",
    "    return Img2Seq(img_encoder, adaptor, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "model = make_model(10000, 512, 512, N=4, drops=0.1, heads=8)    # ~203MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {}
   },
   "source": [
    "## Load modified state_dict into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "sd = torch.load(PATH/'models/word_hw_sp10k.pth', map_location=device)   # ~600MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare names of saved sd parameters\n",
    "# list(sd['model'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare names of model parameters\n",
    "# names = []\n",
    "# for name,weight in learn.model.named_parameters():\n",
    "#     names.append(name)\n",
    "\n",
    "# names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lpe_sd to help figure out the issue with LearnedPositionalEmbeddings\n",
    "\n",
    "# from collections import OrderedDict\n",
    "# lpe_sd = OrderedDict()\n",
    "# lpe_sd['embed.weight'] = sd['model']['transformer.embed.embed.weight']\n",
    "# lpe_sd['rows.weight'] = sd['model']['transformer.embed.rows.weight']\n",
    "# lpe_sd['w_cols.weight'] = sd['model']['transformer.embed.w_cols.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "new_sd = OrderedDict()\n",
    "lin_pattern = re.compile(\"(linears\\.\\d)\")\n",
    "sub_pattern = re.compile(\"(sublayer\\.\\d\\.norm)\")\n",
    "\n",
    "for k,v in sd['model'].items():\n",
    "    if 'linears' in k:\n",
    "        fix_k = lin_pattern.sub(lambda x: f'linear{x.group(0)[-1]}', k)\n",
    "        new_sd[fix_k] = v\n",
    "    elif 'sublayer' in k:\n",
    "        fix_k = sub_pattern.sub(lambda x: f'norm{x.group(0)[-6]}', k)\n",
    "        new_sd[fix_k] = v\n",
    "    else:\n",
    "        new_sd[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {},
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sd,new_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, PATH/'baseline.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.backends.quantized.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# original size\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## statically quantize resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### fuse modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "modules_to_fuse = [\n",
    "    ['base.0', 'base.1', 'base.2'],\n",
    "    ['base.4.0.conv1','base.4.0.bn1','base.4.0.relu'],['base.4.0.conv2','base.4.0.bn2'],\n",
    "    ['base.4.1.conv1','base.4.1.bn1','base.4.1.relu'],['base.4.1.conv2','base.4.1.bn2'],\n",
    "    ['base.5.0.conv1','base.5.0.bn1','base.5.0.relu'],['base.5.0.conv2','base.5.0.bn2'],\n",
    "    ['base.5.0.downsample.0','base.5.0.downsample.1'],\n",
    "    ['base.5.1.conv1','base.5.1.bn1','base.5.1.relu'],['base.5.1.conv2','base.5.1.bn2'],\n",
    "    ['base.6.0.conv1','base.6.0.bn1','base.6.0.relu'],['base.6.0.conv2','base.6.0.bn2'],\n",
    "    ['base.6.0.downsample.0','base.6.0.downsample.1'],\n",
    "    ['base.6.1.conv1','base.6.1.bn1','base.6.1.relu'],['base.6.1.conv2','base.6.1.bn2'],\n",
    "    ['base.7.0.conv1','base.7.0.bn1','base.7.0.relu'],['base.7.0.conv2','base.7.0.bn2'],\n",
    "    ['base.7.0.downsample.0','base.7.0.downsample.1'],\n",
    "    ['base.7.1.conv1','base.7.1.bn1','base.7.1.relu'],['base.7.1.conv2','base.7.1.bn2'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.img_enc.eval()\n",
    "f_img_enc = torch.quantization.fuse_modules(model.img_enc, modules_to_fuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# before fusion\n",
    "print_size_of_model(model.img_enc)\n",
    "\n",
    "# after fusion\n",
    "print_size_of_model(f_img_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qconf = torch.quantization.QConfig(\n",
    "        activation=torch.quantization.default_observer,\n",
    "        weight=torch.quantization.default_per_channel_weight_observer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f_img_enc.qconfig = qconf  #minmax\n",
    "print(f_img_enc.qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f_img_enc.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "print(f_img_enc.qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.quantization.prepare(f_img_enc, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Calibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dummy = torch.rand(1, 3, 512, 512)\n",
    "f_img_enc(dummy); None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# this is just to populate the histogram observers for quantization\n",
    "# don't care about accuracy or rest of model\n",
    "\n",
    "# def q_calibrate(model, dl, n_batches=None):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         n = 0\n",
    "#         for xb,yb in progress_bar(dl):\n",
    "#             _ = model(xb)\n",
    "#             if n_batches and (n >= n_batches): break\n",
    "#             n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# q_calibrate(f_img_enc, data.train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Convert to quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.quantization.convert(f_img_enc, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# after quantization\n",
    "print_size_of_model(f_img_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## dynamically quantize linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "# print(quantized_model.transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "source": [
    "### combine quantized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add back in to model\n",
    "quantized_model.img_enc = f_img_enc\n",
    "\n",
    "# full quantization\n",
    "print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del model,f_img_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = Learner(data, quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# JIT script model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# quantized_model.eval()\n",
    "model_trace = torch.jit.script(quantized_model)  #.eval() necessary to prevent dropout in TorchScript model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_trace.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = Learner(data, model_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {},
    "heading_collapsed": true
   },
   "source": [
    "# Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learn.export()\n",
    "torch.jit.save(model_trace, str(PATH/'fully_quantized_graph_export.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## jit exported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_data(path:PathOrStr, file:PathLikeOrBinaryStream='data.pkl'):\n",
    "    state = torch.load(str(path/file), map_location='cpu')\n",
    "    src = LabelLists.load_state(path, state)\n",
    "    return src.databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_graph(path:PathOrStr, file:PathLikeOrBinaryStream, data:PathLikeOrBinaryStream):\n",
    "    data = load_data(path, data)\n",
    "    graph = torch.jit.load(str(path/file), map_location='cpu')\n",
    "    return Learner(data, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = load_graph(PATH, file='quantized_graph_export.pth', data='data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = open_image(PATH/'test/a03-014.png'); img.resize(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = learn.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(str(res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "# 8.12 s ± 262 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "# quantized\n",
    "# 10.9 s ± 45.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "# traced\n",
    "# 557 ms ± 5.05 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "# quantized, traced\n",
    "# 630 ms ± 7.03 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as tt\n",
    "import Levenshtein as Lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def cer(preds, targs, fn):\n",
    "    bs = targs.size(0)\n",
    "    res = torch.argmax(preds, dim=-1)\n",
    "    error = 0\n",
    "    for i in range(bs):\n",
    "        p = str(fn(res[i]))\n",
    "        t = str(fn(targs[i]))\n",
    "        error += Lev.distance(t, p)/(len(t) or 1)\n",
    "    return error, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._C.set_grad_enabled(False)\n",
    "torch.is_grad_enabled()\n",
    "\n",
    "# this replaces the torch.no_grad context manager in benchmark function!\n",
    "# -> causing errors with benchmark timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model, data, num=10):\n",
    "    x,y = next(iter(data.train_dl))\n",
    "    model.eval()\n",
    "    error = 0\n",
    "    elapsed = 0\n",
    "    for i in progress_bar(range(num)):\n",
    "        #with torch.no_grad():  # this is necessary for jit quantized w/ maxpool2d\n",
    "        start = tt.time()\n",
    "        out = model(x[i:i+1])\n",
    "        end = tt.time()\n",
    "        elapsed += (end-start)\n",
    "        error += cer(out, y[i:i+1], data.y.reconstruct)[0]\n",
    "    print('%.3f CER' % (error/num) + ', ' + '%.2f secs/img' % (elapsed/num) + ', ' + '%.2f MB' % size_of_model(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {'baseline': baseline, 'baseline jit': baseline_jit, 'tfmr quantized': tfmr_q,\n",
    "#           'tfmr quantized jit': tfmr_q_jit, 'full quantized': full, 'full quantized jit': full_jit,\n",
    "#           'full alt': full_alt, 'full alt jit': full_alt_jit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,mod in models.items():\n",
    "#     benchmark(mod, data)\n",
    "#     print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(PATH/'baseline.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_model = torch.jit.load(str(PATH/'fully_quantized_graph_export.pth'), map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='10' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [10/10 01:20<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.037 CER, 8.06 secs/img, 203.60 MB\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.034 CER, 13.50 secs/img, 203.60 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0.077 CER, 18.38 secs/img, 203.60 MB  baseline\n",
    "- 0.062 CER, 13.39 secs/img, 203.60 MB  baseline jit\n",
    "- 0.043 CER, 11.04 secs/img, 100.19 MB  tfmr quantized (dynamic)\n",
    "- 0.040 CER, 12.72 secs/img,  65.46 MB  tfmr quantized jit\n",
    "- 0.037 CER, 11.91 secs/img,  66.68 MB  full quantization\n",
    "- 0.045 CER, 11.31 secs/img,  20.70 MB  full quantization jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantization + TorchScript graphing yields ~10x decrease in model size 203.60MB to 20.70MB\n",
    "and processing performance increase of 63% from 18.38 secs/img to 11.31 secs/img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_1.4] *",
   "language": "python",
   "name": "conda-env-pytorch_1.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "275px",
    "left": "1079px",
    "right": "20px",
    "top": "120px",
    "width": "341px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
